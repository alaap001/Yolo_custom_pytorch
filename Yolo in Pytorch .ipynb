{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### contents of utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import cv2 \n",
    "\n",
    "def predict_transform(prediction,inp_dim,anchors,num_classes,CUDA=True):\n",
    "    \n",
    "    batch_size = prediction.size(0)\n",
    "    stride =  inp_dim // prediction.size(2)\n",
    "    grid_size = inp_dim // stride\n",
    "    bbox_attrs = 5 + num_classes\n",
    "    num_anchors = len(anchors)\n",
    "    \n",
    "    prediction = prediction.view(batch_size, bbox_attrs*num_anchors, grid_size*grid_size)\n",
    "    prediction = prediction.transpose(1,2).contiguous()\n",
    "    prediction = prediction.view(batch_size, grid_size*grid_size*num_anchors, bbox_attrs)\n",
    "    \n",
    "    anchors = [(a[0]/stride, a[1]/stride) for a in anchors]\n",
    "    \n",
    "    prediction[:,:,0]=torch.sigmoid(prediction[:,:,0])\n",
    "    prediction[:,:,1]=torch.sigmoid(prediction[:,:,1])\n",
    "    prediction[:,:,4]=torch.sigmoid(prediction[:,:,4])\n",
    "    \n",
    "    grid=np.arange(grid_size)\n",
    "    a,b=np.meshgrid(grid,grid)\n",
    "    \n",
    "    x_offset=torch.FloatTensor(a).view(-1,1)\n",
    "    y_offset=torch.FloatTensor(b).view(-1,1)\n",
    "    \n",
    "    if CUDA:\n",
    "        x_offset = x_offset.cuda()\n",
    "        y_offset = y_offset.cuda()\n",
    "    \n",
    "    x_y_offset = torch.cat((x_offset, y_offset), 1).repeat(1,num_anchors).view(-1,2).unsqueeze(0)\n",
    "\n",
    "    prediction[:,:,:2] += x_y_offset\n",
    "    \n",
    "    #log space transform height and the width\n",
    "    anchors = torch.FloatTensor(anchors)\n",
    "\n",
    "    if CUDA:\n",
    "        anchors = anchors.cuda()\n",
    "\n",
    "    anchors = anchors.repeat(grid_size*grid_size, 1).unsqueeze(0)\n",
    "    prediction[:,:,2:4] = torch.exp(prediction[:,:,2:4])*anchors\n",
    "\n",
    "    prediction[:,:,5: 5 + num_classes] = torch.sigmoid((prediction[:,:, 5 : 5 + num_classes]))\n",
    "    \n",
    "    prediction[:,:,:4] *= stride\n",
    "    \n",
    "    return prediction\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results(predictions,confidence,num_classes,nms_conf=0.4):\n",
    "    #Our prediction tensor contains information about B x 10647 bounding boxes\n",
    "    conf_mask=(predictions[:,:,4] > confidence).float().unsqueeze(2)\n",
    "    prediction=prediction*conf_mask\n",
    "    box_corner = prediction.new(prediction.shape)\n",
    "    box_corner[:,:,0] = (prediction[:,:,0] - prediction[:,:,2]/2)\n",
    "    box_corner[:,:,1] = (prediction[:,:,1] - prediction[:,:,3]/2)\n",
    "    box_corner[:,:,2] = (prediction[:,:,0] + prediction[:,:,2]/2) \n",
    "    box_corner[:,:,3] = (prediction[:,:,1] + prediction[:,:,3]/2)\n",
    "    prediction[:,:,:4] = box_corner[:,:,:4]\n",
    "    \n",
    "    batch_size = prediction.size(0)\n",
    "\n",
    "    write = False\n",
    "    \n",
    "    for ind in range(batch_size):\n",
    "        image_pred=prediction[ind]\n",
    "        max_conf,max_conf_score=torch.max(image_pred[:,5:5+num_classes],1)\n",
    "        max_conf = max_conf.float().unsqueeze(1)\n",
    "        max_conf_score = max_conf_score.float().unsqueeze(1)\n",
    "        seq = (image_pred[:,:5], max_conf, max_conf_score)\n",
    "        image_pred = torch.cat(seq, 1)\n",
    "        \n",
    "        non_zero_ind =  (torch.nonzero(image_pred[:,4]))\n",
    "        try:\n",
    "            image_pred=image_pred[non_zero_ind.squeeze(),:].view(-1,7)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        if image_pred_.shape[0] == 0:\n",
    "            continue \n",
    "            \n",
    "        img_classes=unique(image_pred[:,-1])\n",
    "        \n",
    "        for cls in img_classes:\n",
    "            \n",
    "\n",
    "def unique(tensor):\n",
    "    tensor_np=tensor.cpu().numpy()\n",
    "    unique_np=np.unique(tensor_np)\n",
    "    unique_tensor=torch.from_numpy(unique_np)\n",
    "    tensor_res=tensor.new(unique_tensor.shape)\n",
    "    tensor_res.copy_(unique_tensor)\n",
    "    return tensor_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # t = torch.randn(2,3)\n",
    "# # t.size(0)\n",
    "# # # t.view?\n",
    "# # t.view(1,6)\n",
    "# grid=np.arange(5)\n",
    "# a,b = np.meshgrid(grid,grid)\n",
    "\n",
    "# x_offset=torch.FloatTensor(a).view(-1,1)\n",
    "# y_offset=torch.FloatTensor(b).view(-1,1)\n",
    "\n",
    "# torch.cat((x_offset,y_offset),1).repeat(1,3).view(-1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "### contents of darknet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "# from util import * \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_cfg(cfgfile):\n",
    "    # parse config to store every block as a dict\n",
    "    file=open(cfgfile,'r')\n",
    "    lines = file.read().split('\\n')\n",
    "    lines = [x for x in lines if len(x)>0]\n",
    "    lines = [x for x in lines if x[0] != '#']\n",
    "    lines = [x.rstrip().lstrip() for x in lines]\n",
    "    \n",
    "    block={}\n",
    "    blocks=[]\n",
    "    \n",
    "    for line in lines:\n",
    "        if line[0]=='[':\n",
    "            if len(block)!=0:\n",
    "                blocks.append(block)\n",
    "                block={}\n",
    "            block['type']=line[1:-1].rstrip()\n",
    "        else:\n",
    "            key,value = line.split('=')\n",
    "            block[key.rstrip()]=value.lstrip()\n",
    "    blocks.append(block)\n",
    "    return blocks\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<i><b> We have 5 types of layers in the list (mentioned above). PyTorch provides pre-built layers for types convolutional and upsample. We will have to write our own modules for the rest of the layers by extending the nn.Module class.</b></i>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.ModuleList\n",
    "\n",
    "Our function will return a nn.ModuleList. This class is almost like a normal list containing nn.Module objects. However, when we add nn.ModuleList as a member of a nn.Module object (i.e. when we add modules to our network), all the parameters of nn.Module objects (modules) inside the nn.ModuleList are added as parameters of the nn.Module object (i.e. our network, which we are adding the nn.ModuleList as a member of) as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmptyLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmptyLayer, self).__init__()\n",
    "        \n",
    "class DetectionLayer(nn.Module):\n",
    "    def __init__(self,anchors):\n",
    "        super().__init__()\n",
    "        self.anchors = anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_modules(blocks):\n",
    "    net_info = blocks[0]\n",
    "    module_list = nn.ModuleList()\n",
    "    prev_filters=3\n",
    "    output_filters=[]\n",
    "    for index,x in enumerate(blocks[1:]):\n",
    "        module = nn.Sequential()\n",
    "        # check the type of block \n",
    "        # create a new module for block and append it to module list\n",
    "        \"\"\"\n",
    "        nn.Sequential class is used to sequentially execute a number of nn.Module objects. \n",
    "        If you look at the cfg, you will realize a block may contain more than one layer. \n",
    "        For example, a block of type convolutional has a batch norm layer as well as leaky ReLU activation layer in addition to a convolutional layer. \n",
    "        We string together these layers using the nn.Sequential and it's the add_module function. \n",
    "        For example, this is how we create the convolutional and the upsample layers.\n",
    "        \"\"\"\n",
    "        if x['type']=='convolutional':\n",
    "            activation=x['activation']\n",
    "            try:\n",
    "                batch_normalize = int(x['batch_normalize'])\n",
    "                bias = False\n",
    "            except:\n",
    "                batch_normalize = 0\n",
    "                bias = True\n",
    "            \n",
    "            filters = int(x['filters'])\n",
    "            padding = int(x[\"pad\"])\n",
    "            kernel_size = int(x[\"size\"])\n",
    "            stride = int(x[\"stride\"])\n",
    "            \n",
    "            if padding:\n",
    "                pad=(kernel_size-1)//2\n",
    "            else:\n",
    "                pad=0\n",
    "            \n",
    "            # add conv layer\n",
    "            conv = nn.Conv2d(prev_filters,filters, kernel_size, stride,pad,bias=bias)\n",
    "            module.add_module(\"conv_{0}\".format(index),conv)\n",
    "            \n",
    "            # add batch norm\n",
    "            if batch_normalize:\n",
    "                bn=nn.BatchNorm2d(filters)\n",
    "                module.add_module(\"batch_norm_{0}\".format(index),bn)\n",
    "            \n",
    "            # check the activation\n",
    "            if activation == \"leaky\":\n",
    "                activn = nn.LeakyReLU(0.1, inplace = True)\n",
    "                module.add_module(\"leaky_{0}\".format(index), activn)\n",
    "                \n",
    "        # if it's an upsample layer, we use vilinear2dupsampling\n",
    "        elif x['type']=='upsample':\n",
    "            stride=int(x['stride'])\n",
    "            upsample = nn.Upsample(scale_factor=2,mode='bilinear')\n",
    "            module.add_module(\"upsample_{}\".format(index),upsample)\n",
    "            \n",
    "        # let's work on route / shortcut layer\n",
    "        elif x['type']=='route':\n",
    "            x['layers']=x['layers'].split(',')\n",
    "            start=int(x['layers'][0])\n",
    "            try:\n",
    "                end=int(x['layers'][1])\n",
    "            except:\n",
    "                end=0\n",
    "            \n",
    "            #Positive annotation\n",
    "            if start>0:\n",
    "                start=start-index\n",
    "            if end>0:\n",
    "                end=end-index\n",
    "                \n",
    "            \n",
    "            route = EmptyLayer()\n",
    "            module.add_module(\"route_{0}\".format(index),route)\n",
    "            if end<0:\n",
    "                filters=output_filters[index+start] + output_filters[index+end]\n",
    "            else:\n",
    "                filters=output_filters[index+start]\n",
    "        \n",
    "        #shortcut\n",
    "        elif x['type']=='shortcut':\n",
    "            shortcut=EmptyLayer()\n",
    "            module.add_module(\"shortcut_{}\".format(index), shortcut)\n",
    "        \n",
    "        elif x['type']=='yolo':\n",
    "            mask = list(map(int,x['mask'].split(',')))\n",
    "            # mask = [int(x) for x in mask]\n",
    "            anchors = list(map(int,x['anchors'].split(',')))\n",
    "            anchors = [(anchors[i],anchors[i+1]) for i in range(0,len(anchors),2)]\n",
    "            \n",
    "            # use only ones indexed in mask\n",
    "            anchors = [anchors[i] for i in mask]\n",
    "            \n",
    "            # we define a custom Detection Layer\n",
    "            detection = DetectionLayer(anchors)\n",
    "            module.add_module(\"Detection_{}\".format(index),detection)\n",
    "            \n",
    "        module_list.append(module)\n",
    "        prev_filters = filters\n",
    "        output_filters.append(filters)\n",
    "        \n",
    "    return (net_info,module_list) \n",
    "\n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=3, out_features=30, bias=True)\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=3, out_features=30, bias=True)\n",
      "  (1): Linear(in_features=30, out_features=3, bias=True)\n",
      ")\n",
      "ModuleList(\n",
      "  (0): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=30, bias=True)\n",
      "    (1): Linear(in_features=30, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "m = nn.Sequential(nn.Linear(3,30))\n",
    "print(m)\n",
    "m.add_module(\"1\",nn.Linear(30,3))\n",
    "print(m)\n",
    "l=nn.ModuleList()\n",
    "l.append(m)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test create_module function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "block = parse_cfg('./cfg/yolov3.cfg')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_l = create_modules(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's work on forward pass\n",
    "cfgfile='./cfg/yolov3.cfg'\n",
    "\n",
    "class Darknet(nn.Module):\n",
    "    def __init__(self,cfgfile):\n",
    "        super().__init__()\n",
    "        self.blocks = parse_cfg(cfgfile)\n",
    "        self.net_info, self.module_list=create_modules(self.blocks)\n",
    "        \n",
    "    def forward(self,x,CUDA):\n",
    "        modules=self.blocks[1:]\n",
    "        outputs={}\n",
    "        \"\"\"\n",
    "        Since route and shortcut layers need output maps from previous layers, \n",
    "        we cache the output feature maps of every layer in a dict outputs\n",
    "        \"\"\"\n",
    "        \n",
    "        #iterate over modules for forward pass\n",
    "        write=0\n",
    "        for i,module in enumerate(modules):\n",
    "            module_type=(module['type'])\n",
    "            if module_type=='convolutional' or module_type=='upsample':\n",
    "                x=self.module_list[i](x)\n",
    "                \n",
    "            elif module_type=='route':\n",
    "                #handle for route\n",
    "                layers=list(map(int,module['layers']))\n",
    "                if layers[0]>0:\n",
    "                    layers[0]-=i\n",
    "                if len(layers)==1:\n",
    "                    x=outputs[i+(layers[0])]\n",
    "                else:\n",
    "                    if layers[1]>0:\n",
    "                        layers[1]-=i\n",
    "                    map1=outputs[i+layers[0]]\n",
    "                    map2=outputs[i+layers[1]]\n",
    "                    \n",
    "                    x=torch.cat((map1,map2),1)\n",
    "            elif  module_type == \"shortcut\":\n",
    "                from_ = int(module[\"from\"])\n",
    "                x = outputs[i-1] + outputs[i+from_]\n",
    "                \n",
    "            elif module_type=='yolo':\n",
    "                anchors=self.module_list[i][0].anchors\n",
    "                inp_dim=int(self.net_info['height'])\n",
    "                \n",
    "                num_classes=int(module['classes'])\n",
    "                \n",
    "                x = predict_transform(x,inp_dim,anchors,num_classes,CUDA)\n",
    "                print(\"x=\",x.shape)\n",
    "                if not write:\n",
    "                    detections = x\n",
    "                    write = 1\n",
    "                else:\n",
    "                    detections = torch.cat((detections,x),1)\n",
    "            outputs[i]=x\n",
    "        return detections\n",
    "    \n",
    "    def load_weights(self,weightfile):\n",
    "        fp=open(weightfile,'rb')\n",
    "        \n",
    "        #The first 5 values are header information \n",
    "        # 1. Major version number\n",
    "        # 2. Minor Version Number\n",
    "        # 3. Subversion number \n",
    "        # 4,5. Images seen by the network (during training)\n",
    "        \n",
    "        header = np.fromfile(fp,dtype=np.int32,count=5)\n",
    "        self.header = torch.from_numpy(header)\n",
    "        self.seen = self.header[3]\n",
    "        \n",
    "        \n",
    "        weights=np.fromfile(fp,dtype=np.float32)\n",
    "        ptr=0\n",
    "        for i in range(len(self.module_list)):\n",
    "            module_type=self.blocks[i+1]['type']\n",
    "            #If module_type is convolutional load weights\n",
    "            #Otherwise ignore.\n",
    "            if module_type == \"convolutional\":\n",
    "                model = self.module_list[i]\n",
    "                try:\n",
    "                    batch_normalize = int(self.blocks[i+1][\"batch_normalize\"])\n",
    "                except:\n",
    "                    batch_normalize = 0\n",
    "\n",
    "                conv = model[0]\n",
    "                \n",
    "                if batch_normalize:\n",
    "                    bn = model[1]\n",
    "                    num_bn_biases = bn.bias.numel()\n",
    "                    bn_biases=torch.from_numpy(weights[ptr:ptr + num_bn_biases])\n",
    "                    ptr += num_bn_biases\n",
    "                    \n",
    "                    bn_weights=torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
    "                    ptr  += num_bn_biases\n",
    "                    \n",
    "                    bn_running_mean = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
    "                    ptr  += num_bn_biases\n",
    "                \n",
    "                    bn_running_var = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
    "                    ptr  += num_bn_biases\n",
    "                    \n",
    "                    bn_biases = bn_biases.view_as(bn.bias.data)\n",
    "                    bn_weights = bn_weights.view_as(bn_weights.data)\n",
    "                    bn_running_mean = bn_running_mean.view_as(bn.running_mean)\n",
    "                    bn_running_var = bn_running_var.view_as(bn.running_var)\n",
    "                    \n",
    "                    \n",
    "                    # copy the data to a model\n",
    "                    bn.bias.data.copy_(bn_biases)\n",
    "                    bn.weight.data.copy_(bn_weights)\n",
    "                    bn.running_mean.copy_(bn_running_mean)\n",
    "                    bn.running_var.copy_(bn_running_var)\n",
    "                    \n",
    "                else:\n",
    "                    num_biases = conv.bias.numel()\n",
    "                    \n",
    "                    conv_biases = torch.from_numpy(weights[ptr:ptr+num_biases])\n",
    "                    ptr+=num_biases\n",
    "                    \n",
    "                    conv_biases = conv_biases.view_as(conv.bias.data)\n",
    "                    conv.bias.data.copy_(conv_biases)\n",
    "                \n",
    "                \n",
    "                # load weights of conv layer\n",
    "                num_weights = conv.weight.numel()\n",
    "                \n",
    "                conv_weights = torch.from_numpy(weights[ptr:ptr+num_weights])\n",
    "                ptr = ptr + num_weights\n",
    "\n",
    "                conv_weights = conv_weights.view_as(conv.weight.data)\n",
    "                conv.weight.data.copy_(conv_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_input():\n",
    "    img = cv2.imread(\"dog-cycle-car.png\")\n",
    "    img = cv2.resize(img, (608,608))          #Resize to the input dimension\n",
    "    img_ =  img[:,:,::-1].transpose((2,0,1))  # BGR -> RGB | H X W C -> C X H X W \n",
    "    img_ = img_[np.newaxis,:,:,:]/255.0       #Add a channel at 0 (for batch) | Normalise\n",
    "    img_ = torch.from_numpy(img_).float()     #Convert to float\n",
    "    img_ = Variable(img_)                     # Convert to Variable\n",
    "    return img_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2392, 0.2392, 0.2392,  ..., 0.6588, 0.2824, 0.2588],\n",
       "          [0.2392, 0.2392, 0.2392,  ..., 0.6196, 0.2510, 0.2431],\n",
       "          [0.2392, 0.2392, 0.2392,  ..., 0.5569, 0.2196, 0.2431],\n",
       "          ...,\n",
       "          [0.6275, 0.6275, 0.6275,  ..., 0.4078, 0.2431, 0.2078],\n",
       "          [0.6235, 0.6235, 0.6235,  ..., 0.3804, 0.2392, 0.1922],\n",
       "          [0.6235, 0.6235, 0.6235,  ..., 0.3569, 0.2392, 0.1804]],\n",
       "\n",
       "         [[0.2392, 0.2392, 0.2392,  ..., 0.6902, 0.2941, 0.2588],\n",
       "          [0.2392, 0.2392, 0.2392,  ..., 0.6510, 0.2627, 0.2392],\n",
       "          [0.2392, 0.2392, 0.2392,  ..., 0.5882, 0.2314, 0.2353],\n",
       "          ...,\n",
       "          [0.6627, 0.6627, 0.6627,  ..., 0.3922, 0.2275, 0.1922],\n",
       "          [0.6588, 0.6588, 0.6588,  ..., 0.3647, 0.2235, 0.1804],\n",
       "          [0.6588, 0.6588, 0.6588,  ..., 0.3412, 0.2235, 0.1647]],\n",
       "\n",
       "         [[0.2235, 0.2235, 0.2235,  ..., 0.4314, 0.1098, 0.1137],\n",
       "          [0.2235, 0.2235, 0.2235,  ..., 0.3961, 0.0863, 0.1020],\n",
       "          [0.2235, 0.2235, 0.2235,  ..., 0.3490, 0.0627, 0.1059],\n",
       "          ...,\n",
       "          [0.7137, 0.7137, 0.7137,  ..., 0.3804, 0.2157, 0.1804],\n",
       "          [0.7098, 0.7098, 0.7098,  ..., 0.3490, 0.2118, 0.1686],\n",
       "          [0.7098, 0.7098, 0.7098,  ..., 0.3255, 0.2118, 0.1529]]]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_test_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= torch.Size([1, 1083, 85])\n",
      "x= torch.Size([1, 4332, 85])\n",
      "x= torch.Size([1, 17328, 85])\n",
      "tensor([[[1.5619e+01, 1.7705e+01, 1.1273e+02,  ..., 1.3319e-03,\n",
      "          4.0802e-04, 3.6722e-04],\n",
      "         [1.7995e+01, 9.8058e+00, 9.4117e+01,  ..., 3.6005e-04,\n",
      "          5.4206e-04, 9.4166e-04],\n",
      "         [2.2240e+01, 1.3196e+01, 4.0238e+02,  ..., 6.6262e-03,\n",
      "          4.6335e-03, 5.4024e-03],\n",
      "         ...,\n",
      "         [6.0482e+02, 6.0185e+02, 3.5539e+00,  ..., 1.8485e-06,\n",
      "          2.6420e-06, 5.1433e-07],\n",
      "         [6.0296e+02, 6.0191e+02, 7.3237e+00,  ..., 1.8137e-05,\n",
      "          2.7303e-05, 1.1349e-05],\n",
      "         [6.0279e+02, 6.0502e+02, 4.2829e+01,  ..., 8.3372e-06,\n",
      "          1.4732e-05, 1.6987e-05]]], grad_fn=<CatBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = Darknet(\"cfg/yolov3.cfg\")\n",
    "model.load_weights(\"yolov3.weights\")\n",
    "inp = get_test_input()\n",
    "pred = model(inp, torch.cuda.is_available())\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
